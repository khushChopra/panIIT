{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.misc import imread\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file = 'training/training/54.png'\n",
    "img = cv2.imread(img_file, cv2.IMREAD_COLOR)\n",
    "\n",
    "hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "h, s, v = cv2.split(hsv)\n",
    "\n",
    "thresh2 = cv2.adaptiveThreshold(v, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "\n",
    "cv2.imshow('Image-thresh2', thresh2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file = 'training/training/54.png'\n",
    "img = cv2.imread(img_file, cv2.IMREAD_COLOR)\n",
    "\n",
    "hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "h, s, v = cv2.split(hsv)\n",
    "\n",
    "ret,thresh_img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV)\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (4,8))\n",
    "morph_img = cv2.morphologyEx(thresh_img, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "thresh2 = cv2.adaptiveThreshold(v, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "\n",
    "cv2.imshow('Image-thresh2', thresh2)\n",
    "cv2.waitKey(0)\n",
    "cv2.imshow('Image-thresh_img', thresh_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.imshow('Image-morph_img', morph_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extractor\n",
    "def extract_features(image_path, vector_size=32):\n",
    "    image = imread(image_path, mode=\"RGB\")\n",
    "    try:\n",
    "        # Using KAZE, cause SIFT, ORB and other was moved to additional module\n",
    "        # which is adding addtional pain during install\n",
    "        alg = cv2.KAZE_create()\n",
    "        # Dinding image keypoints\n",
    "        kps = alg.detect(image)\n",
    "        # Getting first 32 of them. \n",
    "        # Number of keypoints is varies depend on image size and color pallet\n",
    "        # Sorting them based on keypoint response value(bigger is better)\n",
    "        kps = sorted(kps, key=lambda x: -x.response)[:vector_size]\n",
    "        # computing descriptors vector\n",
    "        kps, dsc = alg.compute(image, kps)\n",
    "        # Flatten all of them in one big vector - our feature vector\n",
    "        dsc = dsc.flatten()\n",
    "        # Making descriptor of same size\n",
    "        # Descriptor vector size is 64\n",
    "        needed_size = (vector_size * 64)\n",
    "        if dsc.size < needed_size:\n",
    "            # if we have less the 32 descriptors then just adding zeros at the\n",
    "            # end of our feature vector\n",
    "            dsc = np.concatenate([dsc, np.zeros(needed_size - dsc.size)])\n",
    "    except cv2.error as e:\n",
    "        print('Error: ', e)\n",
    "        return None\n",
    "\n",
    "    return dsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n"
     ]
    }
   ],
   "source": [
    "data = np.zeros([5000,2048])\n",
    "i =0\n",
    "for file in os.listdir(\"training/training\"):\n",
    "    data[i,:] = extract_features(\"training/training/\" + file)\n",
    "    if i%50==0:\n",
    "        print(i)\n",
    "    i+=1\n",
    "np.savetxt(\"foo.csv\", data, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.1861006e-02 -2.3103799e-03  2.5365690e-02 ...  6.8064687e-06\n",
      "  6.4173023e-06  6.8064687e-06]\n"
     ]
    }
   ],
   "source": [
    "print(extract_features('training/training/52.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_extractor(images_path, pickled_db_path=\"features.pck\"):\n",
    "    files = [os.path.join(images_path, p) for p in sorted(os.listdir(images_path))]\n",
    "\n",
    "    result = {}\n",
    "    for f in files:\n",
    "        print 'Extracting features from image %s' % f\n",
    "        name = f.split('/')[-1].lower()\n",
    "        result[name] = extract_features(f)\n",
    "    \n",
    "    # saving all our feature vectors in pickled file\n",
    "    with open(pickled_db_path, 'w') as fp:\n",
    "pickle.dump(result, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
